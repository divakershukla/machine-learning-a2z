{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSogIFZwb1NkGY/YVTz+dH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divakershukla/machine-learning-a2z/blob/main/Tokenization_in_Spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Tokenization is a process of splitting text into meaningful segments **\n",
        "\n",
        "\n",
        "\n",
        "Example:- \"The rapid advancements in artificial intelligence, particularly in the field of natural language processing, have significantly impacted how we interact with technology, allowing for more intuitive and personalized user experiences across various applications like chatbots, search engines, and virtual assistants\""
      ],
      "metadata": {
        "id": "Hlw5dD8QY_xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "ujoajjB-ZxAc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp= spacy.blank(\"en\")"
      ],
      "metadata": {
        "id": "fKDRMB85ZxFF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp(\"The rapid advancements in artificial intelligence, particularly in the field of natural language processing, have significantly impacted how we interact with technology, allowing for more intuitive and personalized user experiences across various applications like chatbots, search engines, and virtual assistants\")\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aOAF503ZNUe",
        "outputId": "c04c917f-8674-4534-b0f4-1d3490ca50ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The\n",
            "rapid\n",
            "advancements\n",
            "in\n",
            "artificial\n",
            "intelligence\n",
            ",\n",
            "particularly\n",
            "in\n",
            "the\n",
            "field\n",
            "of\n",
            "natural\n",
            "language\n",
            "processing\n",
            ",\n",
            "have\n",
            "significantly\n",
            "impacted\n",
            "how\n",
            "we\n",
            "interact\n",
            "with\n",
            "technology\n",
            ",\n",
            "allowing\n",
            "for\n",
            "more\n",
            "intuitive\n",
            "and\n",
            "personalized\n",
            "user\n",
            "experiences\n",
            "across\n",
            "various\n",
            "applications\n",
            "like\n",
            "chatbots\n",
            ",\n",
            "search\n",
            "engines\n",
            ",\n",
            "and\n",
            "virtual\n",
            "assistants\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "nlp= spacy.blank(\"en\")\n",
        "\n",
        "**Text -> NLP[Tokenizer]-> doc**"
      ],
      "metadata": {
        "id": "4S33lq_WasEj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc[-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-F6KvcbZLka",
        "outputId": "7cea73c4-7832-467a-e499-5231d25aaf8a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "assistants"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp(\"Let's go to N.Y.!\")"
      ],
      "metadata": {
        "id": "FTCS6Rc6bkPj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYu26h0lcS1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "79fwpD2kcRlj",
        "outputId": "714a5c02-ae7d-424f-8c90-28cb08ee633d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let\n",
            "'s\n",
            "go\n",
            "to\n",
            "N.Y.\n",
            "!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How tiockenization working **\n",
        "\n",
        "1: Split by prefix ($ ( \"\n",
        "2: Exception 's\n",
        "3: Suffix km) !\"\n",
        "4: Suffix\n",
        "5: Exception\n",
        "\n"
      ],
      "metadata": {
        "id": "F2hj3hg0chsQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "ht3hVjVzcm58",
        "outputId": "27637972-0495-4851-870a-e813b40987d4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>spacy.lang.en.English</b><br/>def __call__(text: Union[str, Doc], *, disable: Iterable[str]=SimpleFrozenList(), component_cfg: Optional[Dict[str, Dict[str, Any]]]=None) -&gt; Doc</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/spacy/lang/en/__init__.py</a>A text-processing pipeline. Usually you&#x27;ll load this once per process,\n",
              "and pass the instance around your application.\n",
              "\n",
              "Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
              "    object and processing pipeline.\n",
              "lang (str): IETF language code, such as &#x27;en&#x27;.\n",
              "\n",
              "DOCS: https://spacy.io/api/language</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 22);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[1:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33fIO1RUdgUn",
        "outputId": "c8ad5e36-34f2-440f-96e4-d28ddda79a0d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'s go to N.Y."
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp(\"I love  $ to spend.\")"
      ],
      "metadata": {
        "id": "T4ZvcHuWdmLC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token0= doc[0]\n",
        "token0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsOTtr-Sd6ov",
        "outputId": "e77cf4ae-4ea5-45e2-d774-3cefdb7e99b9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(token0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in3IicyZeElF",
        "outputId": "a892c38f-dc77-46fd-83b3-e633fefc0f1e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['_',\n",
              " '__bytes__',\n",
              " '__class__',\n",
              " '__delattr__',\n",
              " '__dir__',\n",
              " '__doc__',\n",
              " '__eq__',\n",
              " '__format__',\n",
              " '__ge__',\n",
              " '__getattribute__',\n",
              " '__gt__',\n",
              " '__hash__',\n",
              " '__init__',\n",
              " '__init_subclass__',\n",
              " '__le__',\n",
              " '__len__',\n",
              " '__lt__',\n",
              " '__ne__',\n",
              " '__new__',\n",
              " '__pyx_vtable__',\n",
              " '__reduce__',\n",
              " '__reduce_ex__',\n",
              " '__repr__',\n",
              " '__setattr__',\n",
              " '__sizeof__',\n",
              " '__str__',\n",
              " '__subclasshook__',\n",
              " '__unicode__',\n",
              " 'ancestors',\n",
              " 'check_flag',\n",
              " 'children',\n",
              " 'cluster',\n",
              " 'conjuncts',\n",
              " 'dep',\n",
              " 'dep_',\n",
              " 'doc',\n",
              " 'ent_id',\n",
              " 'ent_id_',\n",
              " 'ent_iob',\n",
              " 'ent_iob_',\n",
              " 'ent_kb_id',\n",
              " 'ent_kb_id_',\n",
              " 'ent_type',\n",
              " 'ent_type_',\n",
              " 'get_extension',\n",
              " 'has_dep',\n",
              " 'has_extension',\n",
              " 'has_head',\n",
              " 'has_morph',\n",
              " 'has_vector',\n",
              " 'head',\n",
              " 'i',\n",
              " 'idx',\n",
              " 'iob_strings',\n",
              " 'is_alpha',\n",
              " 'is_ancestor',\n",
              " 'is_ascii',\n",
              " 'is_bracket',\n",
              " 'is_currency',\n",
              " 'is_digit',\n",
              " 'is_left_punct',\n",
              " 'is_lower',\n",
              " 'is_oov',\n",
              " 'is_punct',\n",
              " 'is_quote',\n",
              " 'is_right_punct',\n",
              " 'is_sent_end',\n",
              " 'is_sent_start',\n",
              " 'is_space',\n",
              " 'is_stop',\n",
              " 'is_title',\n",
              " 'is_upper',\n",
              " 'lang',\n",
              " 'lang_',\n",
              " 'left_edge',\n",
              " 'lefts',\n",
              " 'lemma',\n",
              " 'lemma_',\n",
              " 'lex',\n",
              " 'lex_id',\n",
              " 'like_email',\n",
              " 'like_num',\n",
              " 'like_url',\n",
              " 'lower',\n",
              " 'lower_',\n",
              " 'morph',\n",
              " 'n_lefts',\n",
              " 'n_rights',\n",
              " 'nbor',\n",
              " 'norm',\n",
              " 'norm_',\n",
              " 'orth',\n",
              " 'orth_',\n",
              " 'pos',\n",
              " 'pos_',\n",
              " 'prefix',\n",
              " 'prefix_',\n",
              " 'prob',\n",
              " 'rank',\n",
              " 'remove_extension',\n",
              " 'right_edge',\n",
              " 'rights',\n",
              " 'sent',\n",
              " 'sent_start',\n",
              " 'sentiment',\n",
              " 'set_extension',\n",
              " 'set_morph',\n",
              " 'shape',\n",
              " 'shape_',\n",
              " 'similarity',\n",
              " 'subtree',\n",
              " 'suffix',\n",
              " 'suffix_',\n",
              " 'tag',\n",
              " 'tag_',\n",
              " 'tensor',\n",
              " 'text',\n",
              " 'text_with_ws',\n",
              " 'vector',\n",
              " 'vector_norm',\n",
              " 'vocab',\n",
              " 'whitespace_']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(token0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVDdoj_zeSyL",
        "outputId": "d0b2a604-2041-445d-ab58-84cc22bf7af0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.token.Token"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token0.is_alpha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn27OrFEeakj",
        "outputId": "c143f9e5-ccf0-4ed5-f0e4-893a67de8b61"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token0.like_num"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3efmU5kecyt",
        "outputId": "d4edbc76-0846-4709-b1d9-52759382ecb3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token3= doc[3]"
      ],
      "metadata": {
        "id": "eNAZBHyaemvH"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYtq6XfueqnK",
        "outputId": "eba7349b-b1ac-4035-ce42-b4e4b7e75a95"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "$"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token3.is_currency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gn7id53meut4",
        "outputId": "e0a389e5-983e-4a4b-a52e-ecf359012068"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token, \"==> \", \"index: \", token.i, \"is_alpha\", token.is_alpha,\"is_punct\", token.is_punct, \"like_num\", token.like_num, \"iscurrency\", token.is_currency)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-Elrb2NetOD",
        "outputId": "88c12cad-ee97-4360-d7d0-3f8346259ad5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I ==>  index:  0 is_alpha True is_punct False like_num False iscurrency False\n",
            "love ==>  index:  1 is_alpha True is_punct False like_num False iscurrency False\n",
            "  ==>  index:  2 is_alpha False is_punct False like_num False iscurrency False\n",
            "$ ==>  index:  3 is_alpha False is_punct False like_num False iscurrency True\n",
            "to ==>  index:  4 is_alpha True is_punct False like_num False iscurrency False\n",
            "spend ==>  index:  5 is_alpha True is_punct False like_num False iscurrency False\n",
            ". ==>  index:  6 is_alpha False is_punct True like_num False iscurrency False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "                                        **Use case: Find email present in a document **"
      ],
      "metadata": {
        "id": "PqSOwNYChva6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"students.txt\") as f:\n",
        "  text=f.readlines()\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7cYT8iZf-YM",
        "outputId": "0551c600-57f7-46ef-de03-b4fac30cd4db"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Dayton high school, 8th grade students information\\n',\n",
              " '==================================================\\n',\n",
              " '\\n',\n",
              " 'Name\\tbirth day   \\temail\\n',\n",
              " '-----\\t------------\\t------\\n',\n",
              " 'Virat   5 June, 1882    virat@kohli.com\\n',\n",
              " 'Maria\\t12 April, 2001  maria@sharapova.com\\n',\n",
              " 'Serena  24 June, 1998   serena@williams.com \\n',\n",
              " 'Joe      1 May, 1997    joe@root.com\\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Array to big blog of text**"
      ],
      "metadata": {
        "id": "-8e_w89tgxEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text= ' '.join(text)"
      ],
      "metadata": {
        "id": "4PtAGEhXgtV-"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "1DAsM1u0f750",
        "outputId": "3ae60f04-fe64-44b0-af7a-37a59959eb46"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Dayton high school, 8th grade students information\\n ==================================================\\n \\n Name\\tbirth day   \\temail\\n -----\\t------------\\t------\\n Virat   5 June, 1882    virat@kohli.com\\n Maria\\t12 April, 2001  maria@sharapova.com\\n Serena  24 June, 1998   serena@williams.com \\n Joe      1 May, 1997    joe@root.com\\n \\n \\n \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc=nlp(text)\n",
        "emails=[]"
      ],
      "metadata": {
        "id": "D0jSFcRZfm2W"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  if token.like_email:\n",
        "    emails.append(token)\n",
        "emails"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8zoTvpHhApw",
        "outputId": "141ede29-759f-4d3c-9867-5d00b921d1e9"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[virat@kohli.com, maria@sharapova.com, serena@williams.com, joe@root.com]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Customize Tokenization**"
      ],
      "metadata": {
        "id": "r3-Jrk5CiSS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp(\"gemme high school, 8th grade students information\")"
      ],
      "metadata": {
        "id": "OU_KRC4FiXVl"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tockes= [token.text for token in doc]\n",
        "tockes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZiHXrTmio_s",
        "outputId": "bf8d71fe-e979-475e-ca8d-0d0fd6ff10d2"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gemme', 'high', 'school', ',', '8th', 'grade', 'students', 'information']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.symbols import ORTH\n",
        "\n",
        "nlp.tokenizer.add_special_case(\"gemme\",[\n",
        "    {ORTH: \"gem\"},\n",
        "    {ORTH: \"me\"}\n",
        "])"
      ],
      "metadata": {
        "id": "YrPF11rCiyfX"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp(\"gemme high school, 8th grade students information\")\n",
        "tockes= [token.text for token in doc]\n",
        "tockes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrECd66-jXsS",
        "outputId": "e9c586d9-82b5-45c8-8be1-4e269ee75875"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gem', 'me', 'high', 'school', ',', '8th', 'grade', 'students', 'information']"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp(\"Hello! This is a test-string with special characters: @#$%^&*(), punctuation marks like. Let's tokenize it! \")"
      ],
      "metadata": {
        "id": "WC-4EkDZjzTK"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "DHYW0E1xkvbC",
        "outputId": "e1ff940d-ae72-4335-e02e-87962b22bceb"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-f9b8c240a27c>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: `nlp.add_pipe('sentencizer')`. Alternatively, add the dependency parser or sentence recognizer, or set sentence boundaries by setting `doc[i].is_sent_start`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As pipeline is blank , we need to add componenet**"
      ],
      "metadata": {
        "id": "feWjH2pElPsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe('sentencizer')"
      ],
      "metadata": {
        "id": "nbhu-GEtlELt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for sentence in doc.sents:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxeCb1Gula1e",
        "outputId": "8abf28dc-4103-4bf8-f300-e6dc9229e322"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello!\n",
            "This is a test-string with special characters: @#$%^&*(), punctuation marks like.\n",
            "Let's tokenize it!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Excercise**"
      ],
      "metadata": {
        "id": "CvqQGv8AnKkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise\n",
        "(1) Think stats is a free book to study statistics (https://greenteapress.com/thinkstats2/thinkstats2.pdf)\n",
        "\n",
        "This book has references to many websites from where you can download free datasets. You are an NLP engineer working for some company and you want to collect all dataset websites from this book. To keep exercise simple you are given a paragraph from this book and you want to grab all urls from this paragraph using spacy"
      ],
      "metadata": {
        "id": "Trxi0YgtnO7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text='''\n",
        "Look for data to help you address the question. Governments are good\n",
        "sources because data from public research is often freely available. Good\n",
        "places to start include http://www.data.gov/, and http://www.science.\n",
        "gov/, and in the United Kingdom, http://data.gov.uk/.\n",
        "Two of my favorite data sets are the General Social Survey at http://www3.norc.org/gss+website/,\n",
        "and the European Social Survey at http://www.europeansocialsurvey.org/.\n",
        "'''"
      ],
      "metadata": {
        "id": "x4nULgJ6nNhS"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc= nlp(text)"
      ],
      "metadata": {
        "id": "XJvOfNNJnbAS"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "URL= []\n",
        "for token in doc:\n",
        "  if token.like_url:\n",
        "    URL.append(token)\n",
        "\n",
        "URL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZv6167wnfC0",
        "outputId": "20519f82-e150-4f5e-afca-e806d640a703"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[http://www.data.gov/,\n",
              " http://www.science,\n",
              " http://data.gov.uk/.,\n",
              " http://www3.norc.org/gss+website/,\n",
              " http://www.europeansocialsurvey.org/.]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(2) Extract all money transaction from below sentence along with currency. Output should be,**"
      ],
      "metadata": {
        "id": "hXHL3v99oGTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
        "\n",
        "doc= nlp(transactions)\n",
        "\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWUTwI8JoAGw",
        "outputId": "f46d5846-4b36-4d04-a8a7-2b684c35dec3"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tony\n",
            "gave\n",
            "two\n",
            "$\n",
            "to\n",
            "Peter\n",
            ",\n",
            "Bruce\n",
            "gave\n",
            "500\n",
            "€\n",
            "to\n",
            "Steve\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "currency=[]\n",
        "\n",
        "for token in doc:\n",
        "  if token.is_currency:\n",
        "    cur=doc[token.i-1].text+ \" \" +doc[token.i].text\n",
        "    currency.append(cur)\n",
        "currency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwjVW41YodlQ",
        "outputId": "b03d86d6-5b02-4105-d266-c7e4f0ff3de2"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['two $', '500 €']"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions = \"Tony gave two $ to Peter, Bruce gave 500 € to Steve\"\n",
        "doc = nlp(transactions)\n",
        "for token in doc:\n",
        "    if token.like_num and doc[token.i+1].is_currency:\n",
        "        print(token.text, doc[token.i+1].text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6gxr7mmoo8C",
        "outputId": "541fb620-1cf3-4d61-cf68-2aec4668a76f"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "two $\n",
            "500 €\n"
          ]
        }
      ]
    }
  ]
}